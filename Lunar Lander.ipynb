{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ced2b1",
   "metadata": {},
   "source": [
    "# Lunar Landing using Reinforcement Learning \n",
    "## Proximal Policy Optimization(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e4e38",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7221de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygame\n",
    "!pip install swig\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eb87a",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium import Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4869a5a",
   "metadata": {},
   "source": [
    "#### Test the Enviornment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d7b43",
   "metadata": {},
   "source": [
    "1. Always define the environment in the same cell as where it is rendered. Due to the way pygame works, once the environment is closed, you need to remake it. \n",
    "\n",
    "2. The environment below is the base environment. It shows what happens when there is not trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode='human')\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b2cf65",
   "metadata": {},
   "source": [
    "We create this wrapper so that during training our model knows that landing between the flagpoles will result in a bigger reward and that landing close to it is desirable over landing further way from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionLandingWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # If the lander has landed (terminated), check landing precision\n",
    "        if terminated:\n",
    "            x_pos = obs[0]  # Horizontal position\n",
    "            \n",
    "            # Landing pad is roughly between x = -0.1 and x = 0.1\n",
    "            # Give bonus reward for landing closer to center\n",
    "            if abs(x_pos) < 0.05:  # Very close to center\n",
    "                reward += 100  # Big bonus\n",
    "            elif abs(x_pos) < 0.1:  # Within landing pad\n",
    "                reward += 50   # Medium bonus\n",
    "            elif abs(x_pos) < 0.2:  # Close to landing pad\n",
    "                reward += 5   # Small bonus\n",
    "            else:  # Far from landing pad\n",
    "                reward -= 50   # Penalty for landing far away\n",
    "                \n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PrecisionLandingWrapper(gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "                     enable_wind=False, wind_power=15.0, turbulence_power=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af295b",
   "metadata": {},
   "source": [
    "You can change timesteps to whatever you want as timesteps is basically the training time here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee81efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", train_env, verbose=1)\n",
    "model.learn(total_timesteps=200000, log_interval=50)\n",
    "model.save(\"ppo_lunar_lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_lunar_lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5347c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "                     enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode='human')\n",
    "\n",
    "obs, info = test_env.reset()\n",
    "for _ in range(5000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = test_env.reset()\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
