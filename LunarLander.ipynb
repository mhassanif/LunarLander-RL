{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ced2b1",
   "metadata": {},
   "source": [
    "# Lunar Landing using Reinforcement Learning \n",
    "## Proximal Policy Optimization(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e4e38",
   "metadata": {},
   "source": [
    "#### Step 1: Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7221de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pygame for rendering\n",
    "!pip install pygame\n",
    "# Install swig for compatibility with some environments\n",
    "!pip install swig \n",
    "# Install box2d environments for gymnasium\n",
    "!pip install gymnasium[box2d] \n",
    "# Install stable_baselines3 for reinforcement learning algorithms\n",
    "!pip install stable_baselines3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eb87a",
   "metadata": {},
   "source": [
    "#### Step 2: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3204d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gymansium for creating and interacting with environments.\n",
    "import gymnasium as gym\n",
    "# Proximal Policy Optimization algorithm for reinforcement learning.\n",
    "from stable_baselines3 import PPO\n",
    "# Wrapper for modifying/extending the environment's behavior/functionality.\n",
    "from gymnasium import Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4869a5a",
   "metadata": {},
   "source": [
    "#### Step 3: Initialize the Lunar Lander environment\n",
    "LunarLander-v3 is a classic control problem where the goal is to land a spacecraft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d7b43",
   "metadata": {},
   "source": [
    "1. Always define the environment in the same cell as where it is rendered. Due to the way pygame works, once the environment is closed, you need to remake it. \n",
    "\n",
    "2. The environment below is the base environment. It shows what happens when there is not trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v3\", \n",
    "    continuous=False,  # Discrete action space (fixed thrust levels).\n",
    "    gravity=-10.0,  # Custom gravity setting.\n",
    "    enable_wind=False,  # No wind disturbances.\n",
    "    wind_power=15.0,  # Strength of wind when enabled.\n",
    "    turbulence_power=1.5,  # Intensity of turbulence.\n",
    "    render_mode='human'  # Visual rendering for human observation.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560eb77",
   "metadata": {},
   "source": [
    "#### Step 4: Run the environment with random actions (simulation)\n",
    "This is a test loop to observe how the environment behaves with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d2943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()  # Reset the environment to the initial state.\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Sample a random action from the action space.\n",
    "    observation, reward, terminated, truncated, info = env.step(action)  # Take the action and observe results.\n",
    "\n",
    "    if terminated or truncated:  # Check if the episode has ended.\n",
    "        observation, info = env.reset()  # Reset the environment for the next episode.\n",
    "\n",
    "env.close()  # Close the environment to free resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252f8db",
   "metadata": {},
   "source": [
    "#### Step 5: Define a custom wrapper to enhance the reward function\n",
    "Wrappers allow us to modify the environment behavior, such as changing the reward system.\n",
    "\n",
    "We create this wrapper so that during training our model knows that landing between the flagpoles will result in a bigger reward and that landing close to it is desirable over landing further way from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef1ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionLandingWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # If the lander has landed (terminated), check landing precision\n",
    "        if terminated:\n",
    "            x_pos = obs[0]  # Horizontal position\n",
    "            # Landing pad is roughly between x = -0.1 and x = 0.1\n",
    "            # Give bonus reward for landing closer to center\n",
    "            if abs(x_pos) < 0.05:  # Very close to center\n",
    "                reward += 100  # Big bonus\n",
    "            elif abs(x_pos) < 0.1:  # Within landing pad\n",
    "                reward += 50   # Medium bonus\n",
    "            elif abs(x_pos) < 0.2:  # Close to landing pad\n",
    "                reward += 5   # Small bonus\n",
    "            else:  # Far from landing pad\n",
    "                reward -= 50   # Penalty for landing far away\n",
    "                \n",
    "        # # Alt logic: Adjust reward based on landing precision if the episode ends.\n",
    "        # if terminated:\n",
    "        #     x_pos = obs[0]  # Horizontal position of the lander.\n",
    "        #     # Penalize based on distance from pad's center (x=0).\n",
    "        #     reward -= abs(x_pos) * 10  # Reduce reward for imprecise landings.\n",
    "                \n",
    "        return obs, reward, terminated, truncated, info # updated results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbacba8",
   "metadata": {},
   "source": [
    "#### Step 6: Wrap the environment with the custom wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2eaabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env = PrecisionLandingWrapper(gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "                     enable_wind=False, wind_power=15.0, turbulence_power=1.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f486d5",
   "metadata": {},
   "source": [
    "#### Step 7: Train the PPO agent\n",
    "Initialize the PPO model with the wrapped environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce13a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Multi-layer perceptron policy (neural network).\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     wrapped_env,  \u001b[38;5;66;03m# Environment to train on.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Verbosity level (1 for progress updates).\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model for a specified number of time steps.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Adjust timesteps as needed\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mhass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mhass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Convert discrete action from float to long\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\mhass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:505\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    503\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\mhass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:516\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    511\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[0;32m    513\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[1;32m--> 516\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    520\u001b[0m     )\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_torch, data)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Multi-layer perceptron policy (neural network).\n",
    "    wrapped_env,  # Environment to train on.\n",
    "    verbose=1,  # Verbosity level (1 for progress updates).\n",
    ")\n",
    "\n",
    "# Train the model for a specified number of time steps.\n",
    "# Adjust timesteps as needed\n",
    "model.learn(total_timesteps=200000, log_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f26e2",
   "metadata": {},
   "source": [
    "#### Step 8: Save and load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee81efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_lunar_lander\")  # Save the model to a file.\n",
    "loaded_model = PPO.load(\"ppo_lunar_lander\")  # Load the model back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4d4f9",
   "metadata": {},
   "source": [
    "#### Step 9: Evaluate the trained model\n",
    "Test the model by running it in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the environment with rendering enabled\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "wrapped_env = PrecisionLandingWrapper(env)\n",
    "\n",
    "# Reset and evaluate the trained model\n",
    "obs, info = wrapped_env.reset()\n",
    "for _ in range(10000):  # Limit the number of steps to visualize\n",
    "    action, _states = loaded_model.predict(obs)  # Predict the action\n",
    "    obs, reward, terminated, truncated, info = wrapped_env.step(action)  # Take the action and observe\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = wrapped_env.reset()  # Reset on termination\n",
    "\n",
    "wrapped_env.close()  # Ensure resources are released\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
