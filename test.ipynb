{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ced2b1",
   "metadata": {},
   "source": [
    "# Lunar Landing using Reinforcement Learning \n",
    "## Proximal Policy Optimization(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e4e38",
   "metadata": {},
   "source": [
    "#### Step 1: Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7221de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pygame for rendering\n",
    "!pip install pygame\n",
    "# Install swig for compatibility with some environments\n",
    "!pip install swig \n",
    "# Install box2d environments for gymnasium\n",
    "!pip install gymnasium[box2d] \n",
    "# Install stable_baselines3 for reinforcement learning algorithms\n",
    "!pip install stable_baselines3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eb87a",
   "metadata": {},
   "source": [
    "#### Step 2: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gymansium for creating and interacting with environments.\n",
    "import gymnasium as gym\n",
    "# Proximal Policy Optimization algorithm for reinforcement learning.\n",
    "from stable_baselines3 import PPO\n",
    "# Wrapper for modifying/extending the environment's behavior/functionality.\n",
    "from gymnasium import Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4869a5a",
   "metadata": {},
   "source": [
    "#### Step 3: Initialize the Lunar Lander environment\n",
    "LunarLander-v3 is a classic control problem where the goal is to land a spacecraft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d7b43",
   "metadata": {},
   "source": [
    "1. Always define the environment in the same cell as where it is rendered. Due to the way pygame works, once the environment is closed, you need to remake it. \n",
    "\n",
    "2. The environment below is the base environment. It shows what happens when there is not trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v3\", \n",
    "    continuous=False,  # Discrete action space (fixed thrust levels).\n",
    "    gravity=-10.0,  # Custom gravity setting.\n",
    "    enable_wind=False,  # No wind disturbances.\n",
    "    wind_power=15.0,  # Strength of wind when enabled.\n",
    "    turbulence_power=1.5,  # Intensity of turbulence.\n",
    "    render_mode='human'  # Visual rendering for human observation.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560eb77",
   "metadata": {},
   "source": [
    "#### Step 4: Run the environment with random actions (simulation)\n",
    "This is a test loop to observe how the environment behaves with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()  # Reset the environment to the initial state.\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Sample a random action from the action space.\n",
    "    observation, reward, terminated, truncated, info = env.step(action)  # Take the action and observe results.\n",
    "\n",
    "    if terminated or truncated:  # Check if the episode has ended.\n",
    "        observation, info = env.reset()  # Reset the environment for the next episode.\n",
    "\n",
    "env.close()  # Close the environment to free resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252f8db",
   "metadata": {},
   "source": [
    "#### Step 5: Define a custom wrapper to enhance the reward function\n",
    "Wrappers allow us to modify the environment behavior, such as changing the reward system.\n",
    "\n",
    "We create this wrapper so that during training our model knows that landing between the flagpoles will result in a bigger reward and that landing close to it is desirable over landing further way from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionLandingWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # If the lander has landed (terminated), check landing precision\n",
    "        if terminated:\n",
    "            x_pos = obs[0]  # Horizontal position\n",
    "            # Landing pad is roughly between x = -0.1 and x = 0.1\n",
    "            # Give bonus reward for landing closer to center\n",
    "            if abs(x_pos) < 0.05:  # Very close to center\n",
    "                reward += 100  # Big bonus\n",
    "            elif abs(x_pos) < 0.1:  # Within landing pad\n",
    "                reward += 50   # Medium bonus\n",
    "            elif abs(x_pos) < 0.2:  # Close to landing pad\n",
    "                reward += 5   # Small bonus\n",
    "            else:  # Far from landing pad\n",
    "                reward -= 50   # Penalty for landing far away\n",
    "                \n",
    "        # # Alt logic: Adjust reward based on landing precision if the episode ends.\n",
    "        # if terminated:\n",
    "        #     x_pos = obs[0]  # Horizontal position of the lander.\n",
    "        #     # Penalize based on distance from pad's center (x=0).\n",
    "        #     reward -= abs(x_pos) * 10  # Reduce reward for imprecise landings.\n",
    "                \n",
    "        return obs, reward, terminated, truncated, info # updated results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbacba8",
   "metadata": {},
   "source": [
    "#### Step 6: Wrap the environment with the custom wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env = PrecisionLandingWrapper(gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
    "                     enable_wind=False, wind_power=15.0, turbulence_power=1.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f486d5",
   "metadata": {},
   "source": [
    "#### Step 7: Train the PPO agent\n",
    "Initialize the PPO model with the wrapped environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce13a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",  # Multi-layer perceptron policy (neural network).\n",
    "    wrapped_env,  # Environment to train on.\n",
    "    verbose=1,  # Verbosity level (1 for progress updates).\n",
    ")\n",
    "\n",
    "# Train the model for a specified number of time steps.\n",
    "# Adjust timesteps as needed\n",
    "model.learn(total_timesteps=200000, log_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f26e2",
   "metadata": {},
   "source": [
    "#### Step 8: Save and load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee81efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_lunar_lander\")  # Save the model to a file.\n",
    "loaded_model = PPO.load(\"ppo_lunar_lander\")  # Load the model back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4d4f9",
   "metadata": {},
   "source": [
    "#### Step 9: Evaluate the trained model\n",
    "Test the model by running it in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the environment with rendering enabled\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human')\n",
    "wrapped_env = PrecisionLandingWrapper(env)\n",
    "\n",
    "# Reset and evaluate the trained model\n",
    "obs, info = wrapped_env.reset()\n",
    "for _ in range(10000):  # Limit the number of steps to visualize\n",
    "    action, _states = loaded_model.predict(obs)  # Predict the action\n",
    "    obs, reward, terminated, truncated, info = wrapped_env.step(action)  # Take the action and observe\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = wrapped_env.reset()  # Reset on termination\n",
    "\n",
    "wrapped_env.close()  # Ensure resources are released\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
